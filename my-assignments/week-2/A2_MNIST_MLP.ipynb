{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multilayer Perceptron in Tensorflow\n",
    "\n",
    "Author: Juvid Aryaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "sys.path.append(\"..\")\n",
    "import utls\n",
    "utls.reset_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of 28$\\times$28 handwritten digits. The dataset comes shipped with tensorflow, so let's load it up and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000,), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juvid/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1238: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = np.random.choice(np.arange(n_train))\n",
    "\n",
    "image = x_train[example]\n",
    "label = y_train[example]\n",
    "\n",
    "plt.imshow(image, cmap = cm.binary)\n",
    "print(\"Digit: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multilayer perceptron, we will discard the spatial correlations and simply flatten the data. A convolutional neural network doesn't do this, and is a smarter choice for image data, but we'll start off simple here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat = x_train.reshape((n_train, -1))\n",
    "x_test_flat = x_test.reshape((n_test, -1))\n",
    "x_train_flat.shape, x_test_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the data labels as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    \"\"\"\n",
    "    Encodes a list of labels ranging between (0-9) as one-hot vectors.\n",
    "    0 -> [1,0,0,0,0,0,0,0,0,0]\n",
    "    9 -> [0,0,0,0,0,0,0,0,0,1]\n",
    "    \"\"\"\n",
    "    one_hot_labels = []\n",
    "    for num in labels:\n",
    "        one_hot = [0.0]*10\n",
    "        one_hot[num] = 1.0\n",
    "        one_hot_labels.append(one_hot)\n",
    "    return np.array(one_hot_labels).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)\n",
    "y_train_oh.shape, y_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0],y_train_oh[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST:\n",
    "    \"\"\"\n",
    "    Define a multilayer perceptron for the MNIST dataset\n",
    "    \n",
    "    Params\n",
    "    ------------\n",
    "    wd_factor : A double, the L2 regularisation factor for model parameters\n",
    "    learning_rate : A double, the learning rate for the Adam optimiser    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, wd_factor, learning_rate):\n",
    "        self.wd_factor = wd_factor # weight decay factor (L2 regulariser)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_pointer = 0 # for mini-batch housekeeping\n",
    "        self.test_pointer = 0\n",
    "        \n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None,784],name=\"input\")\n",
    "        self.ground_truth = tf.placeholder(dtype=tf.float32, shape=[None,10],name=\"ground_truth\")\n",
    "        print(self.input)\n",
    "        \n",
    "        self._build_graph()\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Create the multilayer perceptron network using the Adam optimiser and cross entropy loss\n",
    "        \"\"\"\n",
    "        weights = [] # for weight decay\n",
    "        \n",
    "        with tf.variable_scope(\"layers\"):\n",
    "            h = tf.layers.dense(self.input, 512, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='1')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 256, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='2')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 64, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='3')\n",
    "            print(h)\n",
    "            \n",
    "            self.logits = tf.layers.dense(h, 10, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.identity, name='4') # linear output for the loss function\n",
    "            print(self.logits)\n",
    "            self.prediction = tf.nn.softmax(self.logits, name=\"softmax_prediction\")\n",
    "        \n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,\n",
    "                                                                                 labels=self.ground_truth))\n",
    "            self.loss += self.weight_decay() # penalise weights with L2 norm\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def weight_decay(self):\n",
    "        \"\"\"\n",
    "        Append the L2 penalty onto the loss function\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for v in tf.global_variables():\n",
    "            if 'Adam' in v.name:\n",
    "                continue # do not punish optimizer variables\n",
    "            elif 'kernel' in v.name:\n",
    "                loss += self.wd_factor * tf.nn.l2_loss(v)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def train_minibatch(self, samples, labels, batch_size):\n",
    "        \"\"\"\n",
    "        Take a mini-batch from the training dataset\n",
    "        \"\"\"\n",
    "        if self.train_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.train_pointer: self.train_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer += batch_size\n",
    "        else:\n",
    "            samples_minibatch = samples[self.train_pointer:]\n",
    "            labels_minibatch = labels[self.train_pointer:]\n",
    "            self.train_pointer += 0 # reset\n",
    "        return samples_minibatch, labels_minibatch\n",
    "    \n",
    "    def train(self, train_samples, train_labels, train_batch_size, iteration_steps, import_from_previous=False):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Params\n",
    "        ----------\n",
    "        \n",
    "        train_samples: A numpy matrix containing the training data\n",
    "        train_labels: A numpy vector containing the labels corresponding to train_samples\n",
    "        train_batch_size: An int. The number of data points per mini-batch\n",
    "        iteration_steps: An int. The number of mini-batch training steps\n",
    "        import_from_previous: A bool. Load a previous model and train\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        \n",
    "        losses: A numpy array, where the first column is the mini-batch index, and the \n",
    "                second column is the loss function\n",
    "        \"\"\"\n",
    "        print('Start training....')\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if import_from_previous:\n",
    "                saver = tf.train.import_meta_graph(\"./model.meta\") # import the model\n",
    "                saver.restore(sess, './model') # populate with weights  \n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                saver = tf.train.Saver() # for saving models\n",
    "            \n",
    "            for i in range(iteration_steps):\n",
    "                samples, labels = self.train_minibatch(train_samples, train_labels, train_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels}\n",
    "                _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "                if i % 50 == 0:\n",
    "                    print(\"Minibatch loss at step {}: {}\".format(i, loss))\n",
    "                    losses.append([i, loss])\n",
    "            saver.save(sess, './model') # save the model, generating 4 files\n",
    "        return np.array(losses)\n",
    "    \n",
    "    def test_minibatch(self, samples, labels, batch_size):\n",
    "        \"\"\"\n",
    "        Take a mini-batch from the test dataset\n",
    "        \"\"\"\n",
    "        if self.test_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.test_pointer: self.test_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer += batch_size\n",
    "            end_of_epoch = False\n",
    "        else:\n",
    "            samples_minibatch = samples[self.test_pointer:]\n",
    "            labels_minibatch = labels[self.test_pointer:]\n",
    "            self.test_pointer += 0 # reset\n",
    "            end_of_epoch = True\n",
    "        return samples_minibatch, labels_minibatch, end_of_epoch\n",
    "    \n",
    "    def test(self, test_samples, test_labels, test_batch_size):\n",
    "        \"\"\"\n",
    "        Load a model, feed it test data, and determine the loss\n",
    "        \n",
    "        Params:\n",
    "        ---------------\n",
    "        test_samples: A numpy array, containing the test data\n",
    "        test_labels: A numpy vector, containing the test labels\n",
    "        test_batch_size: An int, the number of data points per mini-batch\n",
    "        \"\"\"\n",
    "        end_of_epoch = False\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(\"./model.meta\") # import the model\n",
    "            saver.restore(sess, './model') # populate with weights            \n",
    "            while not end_of_epoch: # run graph\n",
    "                samples, labels, end_of_epoch = self.test_minibatch(test_samples, \n",
    "                                                                    test_labels, test_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels}                \n",
    "                losses.append(sess.run(self.loss, feed_dict=feed_dict))\n",
    "            print(\"Average test loss: {}\".format(np.mean(losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"layers/1/Tanh:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"layers/2/Tanh:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"layers/3/Tanh:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"layers/4/Identity:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"loss/add_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "WD_FACTOR = 0.0001\n",
    "LEARNING_RATE = 0.001\n",
    "model = MLP_MNIST(WD_FACTOR, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'layers/1/kernel:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/kernel/Adam:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/kernel/Adam_1:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias/Adam:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias/Adam_1:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel/Adam:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel/Adam_1:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias/Adam:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias/Adam_1:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel/Adam:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel/Adam_1:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias/Adam:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias/Adam_1:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel/Adam:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel/Adam_1:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():         \n",
    "        shape = variable.get_shape()        \n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value        \n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550346\n"
     ]
    }
   ],
   "source": [
    "count_trainable_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "ITERATIONS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a learning schedule whereby we gradually reduce the learning rate with iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_schedule = [[2000,0.1],[2000,0.01],[2000,0.001],[4000,0.0001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training....\n",
      "Minibatch loss at step 0: 2.36541748046875\n",
      "Minibatch loss at step 50: 0.3655223250389099\n",
      "Minibatch loss at step 100: 0.2856522500514984\n",
      "Minibatch loss at step 150: 0.27969276905059814\n",
      "Minibatch loss at step 200: 0.2908961772918701\n",
      "Minibatch loss at step 250: 0.2861427068710327\n",
      "Minibatch loss at step 300: 0.206180140376091\n",
      "Minibatch loss at step 350: 0.33531466126441956\n",
      "Minibatch loss at step 400: 0.3109612762928009\n",
      "Minibatch loss at step 450: 0.24839672446250916\n",
      "Minibatch loss at step 500: 0.06588887423276901\n",
      "Minibatch loss at step 550: 0.053477752953767776\n",
      "Minibatch loss at step 600: 0.05122058466076851\n",
      "Minibatch loss at step 650: 0.04970638453960419\n",
      "Minibatch loss at step 700: 0.04842778295278549\n",
      "Minibatch loss at step 750: 0.04724952206015587\n",
      "Minibatch loss at step 800: 0.04613037034869194\n",
      "Minibatch loss at step 850: 0.04505439102649689\n",
      "Minibatch loss at step 900: 0.044014088809490204\n",
      "Minibatch loss at step 950: 0.043003957718610764\n",
      "Minibatch loss at step 1000: 0.042020078748464584\n",
      "Minibatch loss at step 1050: 0.0410584881901741\n",
      "Minibatch loss at step 1100: 0.04011589288711548\n",
      "Minibatch loss at step 1150: 0.03919005021452904\n",
      "Minibatch loss at step 1200: 0.03827924281358719\n",
      "Minibatch loss at step 1250: 0.03738240897655487\n",
      "Minibatch loss at step 1300: 0.03649865835905075\n",
      "Minibatch loss at step 1350: 0.03562745079398155\n",
      "Minibatch loss at step 1400: 0.03476836532354355\n",
      "Minibatch loss at step 1450: 0.033921241760253906\n",
      "Minibatch loss at step 1500: 0.033085886389017105\n",
      "Minibatch loss at step 1550: 0.03226224705576897\n",
      "Minibatch loss at step 1600: 0.03145033121109009\n",
      "Minibatch loss at step 1650: 0.03065018728375435\n",
      "Minibatch loss at step 1700: 0.02986186183989048\n",
      "Minibatch loss at step 1750: 0.029085421934723854\n",
      "Minibatch loss at step 1800: 0.02832106128334999\n",
      "Minibatch loss at step 1850: 0.02756889909505844\n",
      "Minibatch loss at step 1900: 0.026829151436686516\n",
      "Minibatch loss at step 1950: 0.02610192261636257\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.025387445464730263\n",
      "Minibatch loss at step 50: 0.02468590810894966\n",
      "Minibatch loss at step 100: 0.023997485637664795\n",
      "Minibatch loss at step 150: 0.02332240156829357\n",
      "Minibatch loss at step 200: 0.02266077697277069\n",
      "Minibatch loss at step 250: 0.022012824192643166\n",
      "Minibatch loss at step 300: 0.02137869969010353\n",
      "Minibatch loss at step 350: 0.020758531987667084\n",
      "Minibatch loss at step 400: 0.02015245333313942\n",
      "Minibatch loss at step 450: 0.019560541957616806\n",
      "Minibatch loss at step 500: 0.018982945010066032\n",
      "Minibatch loss at step 550: 0.018419696018099785\n",
      "Minibatch loss at step 600: 0.017870841547846794\n",
      "Minibatch loss at step 650: 0.017336353659629822\n",
      "Minibatch loss at step 700: 0.01681625284254551\n",
      "Minibatch loss at step 750: 0.01631048321723938\n",
      "Minibatch loss at step 800: 0.01581897959113121\n",
      "Minibatch loss at step 850: 0.015341728925704956\n",
      "Minibatch loss at step 900: 0.014878615736961365\n",
      "Minibatch loss at step 950: 0.014429583214223385\n",
      "Minibatch loss at step 1000: 0.013994559645652771\n",
      "Minibatch loss at step 1050: 0.013573463074862957\n",
      "Minibatch loss at step 1100: 0.013166182674467564\n",
      "Minibatch loss at step 1150: 0.012772572226822376\n",
      "Minibatch loss at step 1200: 0.012392470613121986\n",
      "Minibatch loss at step 1250: 0.012025680392980576\n",
      "Minibatch loss at step 1300: 0.011672018095850945\n",
      "Minibatch loss at step 1350: 0.011331251822412014\n",
      "Minibatch loss at step 1400: 0.011003135703504086\n",
      "Minibatch loss at step 1450: 0.010687432251870632\n",
      "Minibatch loss at step 1500: 0.010383899323642254\n",
      "Minibatch loss at step 1550: 0.010092264972627163\n",
      "Minibatch loss at step 1600: 0.009812258183956146\n",
      "Minibatch loss at step 1650: 0.009543614462018013\n",
      "Minibatch loss at step 1700: 0.009286079555749893\n",
      "Minibatch loss at step 1750: 0.009039348922669888\n",
      "Minibatch loss at step 1800: 0.008803142234683037\n",
      "Minibatch loss at step 1850: 0.008577194064855576\n",
      "Minibatch loss at step 1900: 0.00836118869483471\n",
      "Minibatch loss at step 1950: 0.0081548560410738\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.007957887835800648\n",
      "Minibatch loss at step 50: 0.007769991643726826\n",
      "Minibatch loss at step 100: 0.007590866181999445\n",
      "Minibatch loss at step 150: 0.007420198060572147\n",
      "Minibatch loss at step 200: 0.007257726974785328\n",
      "Minibatch loss at step 250: 0.007103160489350557\n",
      "Minibatch loss at step 300: 0.006956150755286217\n",
      "Minibatch loss at step 350: 0.006816459354013205\n",
      "Minibatch loss at step 400: 0.006683771964162588\n",
      "Minibatch loss at step 450: 0.006557827349752188\n",
      "Minibatch loss at step 500: 0.006438358686864376\n",
      "Minibatch loss at step 550: 0.006325081922113895\n",
      "Minibatch loss at step 600: 0.006217725574970245\n",
      "Minibatch loss at step 650: 0.006116022355854511\n",
      "Minibatch loss at step 700: 0.006019775755703449\n",
      "Minibatch loss at step 750: 0.005928685422986746\n",
      "Minibatch loss at step 800: 0.005842539481818676\n",
      "Minibatch loss at step 850: 0.005761126056313515\n",
      "Minibatch loss at step 900: 0.005684192758053541\n",
      "Minibatch loss at step 950: 0.005611533764749765\n",
      "Minibatch loss at step 1000: 0.0055429632775485516\n",
      "Minibatch loss at step 1050: 0.005478247068822384\n",
      "Minibatch loss at step 1100: 0.005417223088443279\n",
      "Minibatch loss at step 1150: 0.005359687376767397\n",
      "Minibatch loss at step 1200: 0.005305490456521511\n",
      "Minibatch loss at step 1250: 0.005254408810287714\n",
      "Minibatch loss at step 1300: 0.0052063362672924995\n",
      "Minibatch loss at step 1350: 0.005161062348634005\n",
      "Minibatch loss at step 1400: 0.005118469707667828\n",
      "Minibatch loss at step 1450: 0.005078417249023914\n",
      "Minibatch loss at step 1500: 0.005040740128606558\n",
      "Minibatch loss at step 1550: 0.005005320534110069\n",
      "Minibatch loss at step 1600: 0.004972015041857958\n",
      "Minibatch loss at step 1650: 0.0049407281912863255\n",
      "Minibatch loss at step 1700: 0.004911309573799372\n",
      "Minibatch loss at step 1750: 0.004883675370365381\n",
      "Minibatch loss at step 1800: 0.004857685882598162\n",
      "Minibatch loss at step 1850: 0.004833265673369169\n",
      "Minibatch loss at step 1900: 0.0048103295266628265\n",
      "Minibatch loss at step 1950: 0.0047887228429317474\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.004768407437950373\n",
      "Minibatch loss at step 50: 0.004749267362058163\n",
      "Minibatch loss at step 100: 0.004731243010610342\n",
      "Minibatch loss at step 150: 0.004714227747172117\n",
      "Minibatch loss at step 200: 0.004698166158050299\n",
      "Minibatch loss at step 250: 0.004682974889874458\n",
      "Minibatch loss at step 300: 0.004668578505516052\n",
      "Minibatch loss at step 350: 0.0046548983082175255\n",
      "Minibatch loss at step 400: 0.004641871899366379\n",
      "Minibatch loss at step 450: 0.004629462957382202\n",
      "Minibatch loss at step 500: 0.0046175215393304825\n",
      "Minibatch loss at step 550: 0.004606058821082115\n",
      "Minibatch loss at step 600: 0.0045949844643473625\n",
      "Minibatch loss at step 650: 0.004584258887916803\n",
      "Minibatch loss at step 700: 0.004573793616145849\n",
      "Minibatch loss at step 750: 0.004563598893582821\n",
      "Minibatch loss at step 800: 0.004553649574518204\n",
      "Minibatch loss at step 850: 0.004543885588645935\n",
      "Minibatch loss at step 900: 0.004534303210675716\n",
      "Minibatch loss at step 950: 0.004524878226220608\n",
      "Minibatch loss at step 1000: 0.004515588749200106\n",
      "Minibatch loss at step 1050: 0.004506364930421114\n",
      "Minibatch loss at step 1100: 0.004497217945754528\n",
      "Minibatch loss at step 1150: 0.0044881184585392475\n",
      "Minibatch loss at step 1200: 0.00447904784232378\n",
      "Minibatch loss at step 1250: 0.0044699483551084995\n",
      "Minibatch loss at step 1300: 0.004460853990167379\n",
      "Minibatch loss at step 1350: 0.0044517493806779385\n",
      "Minibatch loss at step 1400: 0.004442663863301277\n",
      "Minibatch loss at step 1450: 0.0044335732236504555\n",
      "Minibatch loss at step 1500: 0.00442451611161232\n",
      "Minibatch loss at step 1550: 0.004415545612573624\n",
      "Minibatch loss at step 1600: 0.004406660795211792\n",
      "Minibatch loss at step 1650: 0.004397856071591377\n",
      "Minibatch loss at step 1700: 0.004389197565615177\n",
      "Minibatch loss at step 1750: 0.0043806773610413074\n",
      "Minibatch loss at step 1800: 0.004372305236756802\n",
      "Minibatch loss at step 1850: 0.0043640825897455215\n",
      "Minibatch loss at step 1900: 0.004356037825345993\n",
      "Minibatch loss at step 1950: 0.004348107613623142\n",
      "Minibatch loss at step 2000: 0.00434033153578639\n",
      "Minibatch loss at step 2050: 0.004332615528255701\n",
      "Minibatch loss at step 2100: 0.004325004294514656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2150: 0.00431734137237072\n",
      "Minibatch loss at step 2200: 0.004309662152081728\n",
      "Minibatch loss at step 2250: 0.004301889333873987\n",
      "Minibatch loss at step 2300: 0.004293903708457947\n",
      "Minibatch loss at step 2350: 0.004285682924091816\n",
      "Minibatch loss at step 2400: 0.004277055151760578\n",
      "Minibatch loss at step 2450: 0.004267947748303413\n",
      "Minibatch loss at step 2500: 0.004258191213011742\n",
      "Minibatch loss at step 2550: 0.004247655160725117\n",
      "Minibatch loss at step 2600: 0.004236198496073484\n",
      "Minibatch loss at step 2650: 0.004223735071718693\n",
      "Minibatch loss at step 2700: 0.00421034125611186\n",
      "Minibatch loss at step 2750: 0.004196323920041323\n",
      "Minibatch loss at step 2800: 0.004182148724794388\n",
      "Minibatch loss at step 2850: 0.0041684359312057495\n",
      "Minibatch loss at step 2900: 0.004155577160418034\n",
      "Minibatch loss at step 2950: 0.004143727943301201\n",
      "Minibatch loss at step 3000: 0.004132854752242565\n",
      "Minibatch loss at step 3050: 0.004122798331081867\n",
      "Minibatch loss at step 3100: 0.004113412462174892\n",
      "Minibatch loss at step 3150: 0.0041045378893613815\n",
      "Minibatch loss at step 3200: 0.004096105229109526\n",
      "Minibatch loss at step 3250: 0.00408798037096858\n",
      "Minibatch loss at step 3300: 0.00408015213906765\n",
      "Minibatch loss at step 3350: 0.0040725222788751125\n",
      "Minibatch loss at step 3400: 0.004065070301294327\n",
      "Minibatch loss at step 3450: 0.004057720303535461\n",
      "Minibatch loss at step 3500: 0.004050459712743759\n",
      "Minibatch loss at step 3550: 0.0040432363748550415\n",
      "Minibatch loss at step 3600: 0.00403603445738554\n",
      "Minibatch loss at step 3650: 0.00402881670743227\n",
      "Minibatch loss at step 3700: 0.0040215798653662205\n",
      "Minibatch loss at step 3750: 0.00401430344209075\n",
      "Minibatch loss at step 3800: 0.004007021430879831\n",
      "Minibatch loss at step 3850: 0.003999772481620312\n",
      "Minibatch loss at step 3900: 0.003992540296167135\n",
      "Minibatch loss at step 3950: 0.003985400777310133\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i, vals in enumerate(learning_schedule):\n",
    "    ITERATIONS = vals[0]\n",
    "    LR = vals[1]\n",
    "    if i == 0:\n",
    "        model.learning_rate = LR\n",
    "        losses = model.train(x_train_flat, y_train_oh, TRAIN_BATCH_SIZE, ITERATIONS)\n",
    "    else:\n",
    "\n",
    "        losses_new = model.train(x_train_flat, y_train_oh, TRAIN_BATCH_SIZE, ITERATIONS, import_from_previous = True)\n",
    "        losses_new[:,0] += losses[-1,0] + TRAIN_BATCH_SIZE\n",
    "        losses = np.vstack((losses,losses_new))\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 123.32523131370544s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time: {}s\".format(end_time - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = losses[:,0]\n",
    "train_loss = losses[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "utls.remove_tex_axis(ax, ytick_fmt=\"%.1f\")\n",
    "ax.plot(iterations, train_loss,'-k')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Average test loss: 2.2502541542053223\n"
     ]
    }
   ],
   "source": [
    "model.test(x_test_flat,y_test_oh, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Test sample digit: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "example = np.random.choice(n_test)\n",
    "\n",
    "sample = x_test_flat[example]\n",
    "label = y_test_oh[example]\n",
    "feed_dict = {model.input: np.expand_dims(sample, axis=0), \n",
    "             model.ground_truth: np.expand_dims(label,axis=0)}\n",
    "\n",
    "digit = np.where(label==1.0)[0][0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"./model.meta\")\n",
    "    saver.restore(sess, './model')\n",
    "    prediction = sess.run(model.prediction, feed_dict = feed_dict)[0]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(2*5,5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "ax = axs[0]\n",
    "utls.remove_tex_axis(ax)\n",
    "ax.imshow(x_test[example],cmap=cm.binary)\n",
    "ax.set_title('Test example')\n",
    "\n",
    "classes = np.arange(10)\n",
    "width = 0.5\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "ax.bar(classes, prediction, width, color='Black')\n",
    "utls.remove_tex_axis(ax,ytick_fmt=\"%.2f\")\n",
    "ax.set_xticks(classes)\n",
    "ax.set_xticklabels([str(x) for x in np.arange(10)])\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Digit class')\n",
    "ax.set_title('Network categorical distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "print('Test sample digit: {}'.format(digit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
