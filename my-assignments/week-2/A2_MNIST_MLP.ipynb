{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multilayer Perceptron in Tensorflow\n",
    "\n",
    "Author: Juvid Aryaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "sys.path.append(\"..\")\n",
    "import utls\n",
    "utls.reset_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of 28$\\times$28 handwritten digits. The dataset comes shipped with tensorflow, so let's load it up and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000,), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juvid/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1238: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = np.random.choice(np.arange(n_train))\n",
    "\n",
    "image = x_train[example]\n",
    "label = y_train[example]\n",
    "\n",
    "plt.imshow(image, cmap = cm.binary)\n",
    "print(\"Digit: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multilayer perceptron, we will discard the spatial correlations and simply flatten the data. A convolutional neural network doesn't do this, and is a smarter choice for image data, but we'll start off simple here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat = x_train.reshape((n_train, -1))\n",
    "x_test_flat = x_test.reshape((n_test, -1))\n",
    "x_train_flat.shape, x_test_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the data labels as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    \"\"\"\n",
    "    Encodes a list of labels ranging between (0-9) as one-hot vectors.\n",
    "    0 -> [1,0,0,0,0,0,0,0,0,0]\n",
    "    9 -> [0,0,0,0,0,0,0,0,0,1]\n",
    "    \"\"\"\n",
    "    one_hot_labels = []\n",
    "    for num in labels:\n",
    "        one_hot = [0.0]*10\n",
    "        one_hot[num] = 1.0\n",
    "        one_hot_labels.append(one_hot)\n",
    "    return np.array(one_hot_labels).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)\n",
    "y_train_oh.shape, y_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0],y_train_oh[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST:\n",
    "    \"\"\"\n",
    "    Define a multilayer perceptron for the MNIST dataset\n",
    "    \n",
    "    Params\n",
    "    ------------\n",
    "    wd_factor : A double, the L2 regularisation factor for model parameters\n",
    "    learning_rate : A double, the learning rate for the Adam optimiser    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, wd_factor, learning_rate):\n",
    "        self.wd_factor = wd_factor # weight decay factor (L2 regulariser)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_pointer = 0 # for mini-batch housekeeping\n",
    "        self.test_pointer = 0\n",
    "        \n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None,784],name=\"input\")\n",
    "        self.ground_truth = tf.placeholder(dtype=tf.float32, shape=[None,10],name=\"ground_truth\")\n",
    "        print(self.input)\n",
    "        \n",
    "        self._build_graph()\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Create the multilayer perceptron network using the Adam optimiser and cross entropy loss\n",
    "        \"\"\"\n",
    "        weights = [] # for weight decay\n",
    "        \n",
    "        with tf.variable_scope(\"layers\"):\n",
    "            h = tf.layers.dense(self.input, 512, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='1')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 256, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='2')\n",
    "            print(h)\n",
    "            \n",
    "            h = tf.layers.dense(h, 64, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='3')\n",
    "            print(h)\n",
    "            \n",
    "            self.logits = tf.layers.dense(h, 10, kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "                               activation=tf.tanh, name='4') # linear output for the loss function\n",
    "            print(self.logits)\n",
    "            self.prediction = tf.nn.softmax(self.logits, name=\"softmax_prediction\")\n",
    "        \n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,\n",
    "                                                                                 labels=self.ground_truth))\n",
    "            self.loss += self.weight_decay() # penalise weights with L2 norm\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def weight_decay(self):\n",
    "        \"\"\"\n",
    "        Append the L2 penalty onto the loss function\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for v in tf.global_variables():\n",
    "            if 'Adam' in v.name:\n",
    "                continue # do not punish optimizer variables\n",
    "            elif 'kernel' in v.name:\n",
    "                loss += self.wd_factor * tf.nn.l2_loss(v)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def train_minibatch(self, samples, labels, batch_size):\n",
    "        \"\"\"\n",
    "        Take a mini-batch from the training dataset\n",
    "        \"\"\"\n",
    "        if self.train_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.train_pointer: self.train_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.train_pointer: self.train_pointer + batch_size]\n",
    "            self.train_pointer += batch_size\n",
    "        else:\n",
    "            samples_minibatch = samples[self.train_pointer:]\n",
    "            labels_minibatch = labels[self.train_pointer:]\n",
    "            self.train_pointer += 0 # reset\n",
    "        return samples_minibatch, labels_minibatch\n",
    "    \n",
    "    def train(self, train_samples, train_labels, train_batch_size, iteration_steps, import_from_previous=False):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Params\n",
    "        ----------\n",
    "        \n",
    "        train_samples: A numpy matrix containing the training data\n",
    "        train_labels: A numpy vector containing the labels corresponding to train_samples\n",
    "        train_batch_size: An int. The number of data points per mini-batch\n",
    "        iteration_steps: An int. The number of mini-batch training steps\n",
    "        import_from_previous: A bool. Load a previous model and train\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        \n",
    "        losses: A numpy array, where the first column is the mini-batch index, and the \n",
    "                second column is the loss function\n",
    "        \"\"\"\n",
    "        print('Start training....')\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if import_from_previous:\n",
    "                saver = tf.train.import_meta_graph(\"./model.meta\") # import the model\n",
    "                saver.restore(sess, './model') # populate with weights  \n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                saver = tf.train.Saver() # for saving models\n",
    "            \n",
    "            for i in range(iteration_steps):\n",
    "                samples, labels = self.train_minibatch(train_samples, train_labels, train_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels}\n",
    "                _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "                if i % 50 == 0:\n",
    "                    print(\"Minibatch loss at step {}: {}\".format(i, loss))\n",
    "                    losses.append([i, loss])\n",
    "            saver.save(sess, './model') # save the model, generating 4 files\n",
    "        return np.array(losses)\n",
    "    \n",
    "    def test_minibatch(self, samples, labels, batch_size):\n",
    "        \"\"\"\n",
    "        Take a mini-batch from the test dataset\n",
    "        \"\"\"\n",
    "        if self.test_pointer + batch_size <= samples.shape[0]:\n",
    "            samples_minibatch = samples[self.test_pointer: self.test_pointer + batch_size]\n",
    "            labels_minibatch = labels[self.test_pointer: self.test_pointer + batch_size]\n",
    "            self.test_pointer += batch_size\n",
    "            end_of_epoch = False\n",
    "        else:\n",
    "            samples_minibatch = samples[self.test_pointer:]\n",
    "            labels_minibatch = labels[self.test_pointer:]\n",
    "            self.test_pointer += 0 # reset\n",
    "            end_of_epoch = True\n",
    "        return samples_minibatch, labels_minibatch, end_of_epoch\n",
    "    \n",
    "    def test(self, test_samples, test_labels, test_batch_size):\n",
    "        \"\"\"\n",
    "        Load a model, feed it test data, and determine the loss\n",
    "        \n",
    "        Params:\n",
    "        ---------------\n",
    "        test_samples: A numpy array, containing the test data\n",
    "        test_labels: A numpy vector, containing the test labels\n",
    "        test_batch_size: An int, the number of data points per mini-batch\n",
    "        \"\"\"\n",
    "        end_of_epoch = False\n",
    "        losses = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(\"./model.meta\") # import the model\n",
    "            saver.restore(sess, './model') # populate with weights            \n",
    "            while not end_of_epoch: # run graph\n",
    "                samples, labels, end_of_epoch = self.test_minibatch(test_samples, \n",
    "                                                                    test_labels, test_batch_size)\n",
    "                feed_dict = {self.input: samples, self.ground_truth: labels}                \n",
    "                losses.append(sess.run(self.loss, feed_dict=feed_dict))\n",
    "            print(\"Average test loss: {}\".format(np.mean(losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"layers/1/Tanh:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"layers/2/Tanh:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"layers/3/Tanh:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"layers/4/Tanh:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"loss/add_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "WD_FACTOR = 0.0001\n",
    "LEARNING_RATE = 0.001\n",
    "model = MLP_MNIST(WD_FACTOR, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'layers/1/kernel:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/kernel/Adam:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/kernel/Adam_1:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias/Adam:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/1/bias/Adam_1:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel/Adam:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/kernel/Adam_1:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias/Adam:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/2/bias/Adam_1:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel/Adam:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/kernel/Adam_1:0' shape=(256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias/Adam:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/3/bias/Adam_1:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel/Adam:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/kernel/Adam_1:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'layers/4/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():         \n",
    "        shape = variable.get_shape()        \n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value        \n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550346\n"
     ]
    }
   ],
   "source": [
    "count_trainable_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "ITERATIONS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a learning schedule whereby we gradually reduce the learning rate with iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_schedule = [[2000,0.1],[2000,0.01],[2000,0.001],[4000,0.0001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training....\n",
      "Minibatch loss at step 0: 2.491102457046509\n",
      "Minibatch loss at step 50: 1.0957317352294922\n",
      "Minibatch loss at step 100: 1.021644115447998\n",
      "Minibatch loss at step 150: 1.0002901554107666\n",
      "Minibatch loss at step 200: 0.9801380038261414\n",
      "Minibatch loss at step 250: 1.0051335096359253\n",
      "Minibatch loss at step 300: 0.9503704905509949\n",
      "Minibatch loss at step 350: 1.008354902267456\n",
      "Minibatch loss at step 400: 1.0059380531311035\n",
      "Minibatch loss at step 450: 0.9671732187271118\n",
      "Minibatch loss at step 500: 0.8679303526878357\n",
      "Minibatch loss at step 550: 0.8652858734130859\n",
      "Minibatch loss at step 600: 0.8635293245315552\n",
      "Minibatch loss at step 650: 0.8619713187217712\n",
      "Minibatch loss at step 700: 0.860521674156189\n",
      "Minibatch loss at step 750: 0.8591464757919312\n",
      "Minibatch loss at step 800: 0.8578277826309204\n",
      "Minibatch loss at step 850: 0.8565553426742554\n",
      "Minibatch loss at step 900: 0.8553225994110107\n",
      "Minibatch loss at step 950: 0.8541253209114075\n",
      "Minibatch loss at step 1000: 0.852960467338562\n",
      "Minibatch loss at step 1050: 0.8518260717391968\n",
      "Minibatch loss at step 1100: 0.8507207036018372\n",
      "Minibatch loss at step 1150: 0.8496431708335876\n",
      "Minibatch loss at step 1200: 0.8485927581787109\n",
      "Minibatch loss at step 1250: 0.8475688099861145\n",
      "Minibatch loss at step 1300: 0.8465710282325745\n",
      "Minibatch loss at step 1350: 0.8455991744995117\n",
      "Minibatch loss at step 1400: 0.8446531295776367\n",
      "Minibatch loss at step 1450: 0.8437325954437256\n",
      "Minibatch loss at step 1500: 0.8428374528884888\n",
      "Minibatch loss at step 1550: 0.8419677019119263\n",
      "Minibatch loss at step 1600: 0.8411232233047485\n",
      "Minibatch loss at step 1650: 0.8403037190437317\n",
      "Minibatch loss at step 1700: 0.8395093679428101\n",
      "Minibatch loss at step 1750: 0.8387398719787598\n",
      "Minibatch loss at step 1800: 0.8379951119422913\n",
      "Minibatch loss at step 1850: 0.8372747898101807\n",
      "Minibatch loss at step 1900: 0.8365787863731384\n",
      "Minibatch loss at step 1950: 0.8359069228172302\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.8352586627006531\n",
      "Minibatch loss at step 50: 0.834634006023407\n",
      "Minibatch loss at step 100: 0.834032416343689\n",
      "Minibatch loss at step 150: 0.8334535360336304\n",
      "Minibatch loss at step 200: 0.832896888256073\n",
      "Minibatch loss at step 250: 0.8323622941970825\n",
      "Minibatch loss at step 300: 0.8318489789962769\n",
      "Minibatch loss at step 350: 0.8313566446304321\n",
      "Minibatch loss at step 400: 0.8308848142623901\n",
      "Minibatch loss at step 450: 0.8304330110549927\n",
      "Minibatch loss at step 500: 0.8300005793571472\n",
      "Minibatch loss at step 550: 0.8295871019363403\n",
      "Minibatch loss at step 600: 0.8291919827461243\n",
      "Minibatch loss at step 650: 0.8288149237632751\n",
      "Minibatch loss at step 700: 0.8284547328948975\n",
      "Minibatch loss at step 750: 0.8281106948852539\n",
      "Minibatch loss at step 800: 0.8277657628059387\n",
      "Minibatch loss at step 850: 0.822529673576355\n",
      "Minibatch loss at step 900: 0.8221606612205505\n",
      "Minibatch loss at step 950: 0.8218613862991333\n",
      "Minibatch loss at step 1000: 0.8215849995613098\n",
      "Minibatch loss at step 1050: 0.8213253617286682\n",
      "Minibatch loss at step 1100: 0.8210805654525757\n",
      "Minibatch loss at step 1150: 0.8208494186401367\n",
      "Minibatch loss at step 1200: 0.8206309676170349\n",
      "Minibatch loss at step 1250: 0.8204245567321777\n",
      "Minibatch loss at step 1300: 0.8202297687530518\n",
      "Minibatch loss at step 1350: 0.8200456500053406\n",
      "Minibatch loss at step 1400: 0.8198719024658203\n",
      "Minibatch loss at step 1450: 0.8197080492973328\n",
      "Minibatch loss at step 1500: 0.8195534944534302\n",
      "Minibatch loss at step 1550: 0.8194077610969543\n",
      "Minibatch loss at step 1600: 0.8192704916000366\n",
      "Minibatch loss at step 1650: 0.819141149520874\n",
      "Minibatch loss at step 1700: 0.8190192580223083\n",
      "Minibatch loss at step 1750: 0.8189045190811157\n",
      "Minibatch loss at step 1800: 0.8187965750694275\n",
      "Minibatch loss at step 1850: 0.8186947703361511\n",
      "Minibatch loss at step 1900: 0.8185980916023254\n",
      "Minibatch loss at step 1950: 0.8038861155509949\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.8032925724983215\n",
      "Minibatch loss at step 50: 0.8030062317848206\n",
      "Minibatch loss at step 100: 0.8028231859207153\n",
      "Minibatch loss at step 150: 0.8026868104934692\n",
      "Minibatch loss at step 200: 0.8025791049003601\n",
      "Minibatch loss at step 250: 0.8024927377700806\n",
      "Minibatch loss at step 300: 0.8024201989173889\n",
      "Minibatch loss at step 350: 0.8023566007614136\n",
      "Minibatch loss at step 400: 0.8022991418838501\n",
      "Minibatch loss at step 450: 0.802246630191803\n",
      "Minibatch loss at step 500: 0.8021981716156006\n",
      "Minibatch loss at step 550: 0.8021532893180847\n",
      "Minibatch loss at step 600: 0.8021112680435181\n",
      "Minibatch loss at step 650: 0.8020721077919006\n",
      "Minibatch loss at step 700: 0.802035391330719\n",
      "Minibatch loss at step 750: 0.8020009398460388\n",
      "Minibatch loss at step 800: 0.8019686341285706\n",
      "Minibatch loss at step 850: 0.8019380569458008\n",
      "Minibatch loss at step 900: 0.8019094467163086\n",
      "Minibatch loss at step 950: 0.8018823862075806\n",
      "Minibatch loss at step 1000: 0.8018568754196167\n",
      "Minibatch loss at step 1050: 0.8018327355384827\n",
      "Minibatch loss at step 1100: 0.8018099665641785\n",
      "Minibatch loss at step 1150: 0.8017884492874146\n",
      "Minibatch loss at step 1200: 0.8017680048942566\n",
      "Minibatch loss at step 1250: 0.8017485737800598\n",
      "Minibatch loss at step 1300: 0.801730215549469\n",
      "Minibatch loss at step 1350: 0.8017127513885498\n",
      "Minibatch loss at step 1400: 0.8016961216926575\n",
      "Minibatch loss at step 1450: 0.8016802072525024\n",
      "Minibatch loss at step 1500: 0.8016648888587952\n",
      "Minibatch loss at step 1550: 0.8016504049301147\n",
      "Minibatch loss at step 1600: 0.8016364574432373\n",
      "Minibatch loss at step 1650: 0.8016231656074524\n",
      "Minibatch loss at step 1700: 0.8016103506088257\n",
      "Minibatch loss at step 1750: 0.801598072052002\n",
      "Minibatch loss at step 1800: 0.8015861511230469\n",
      "Minibatch loss at step 1850: 0.80157470703125\n",
      "Minibatch loss at step 1900: 0.8015636205673218\n",
      "Minibatch loss at step 1950: 0.8015528321266174\n",
      "Start training....\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Minibatch loss at step 0: 0.8015424013137817\n",
      "Minibatch loss at step 50: 0.8015322089195251\n",
      "Minibatch loss at step 100: 0.8015223741531372\n",
      "Minibatch loss at step 150: 0.8015127182006836\n",
      "Minibatch loss at step 200: 0.8015032410621643\n",
      "Minibatch loss at step 250: 0.8014941811561584\n",
      "Minibatch loss at step 300: 0.8014850616455078\n",
      "Minibatch loss at step 350: 0.8014762997627258\n",
      "Minibatch loss at step 400: 0.8014675974845886\n",
      "Minibatch loss at step 450: 0.8014590740203857\n",
      "Minibatch loss at step 500: 0.8014507293701172\n",
      "Minibatch loss at step 550: 0.8014425039291382\n",
      "Minibatch loss at step 600: 0.801434338092804\n",
      "Minibatch loss at step 650: 0.8014264106750488\n",
      "Minibatch loss at step 700: 0.8014185428619385\n",
      "Minibatch loss at step 750: 0.8014106750488281\n",
      "Minibatch loss at step 800: 0.8014028668403625\n",
      "Minibatch loss at step 850: 0.8013952970504761\n",
      "Minibatch loss at step 900: 0.8013877868652344\n",
      "Minibatch loss at step 950: 0.8013802170753479\n",
      "Minibatch loss at step 1000: 0.8013730049133301\n",
      "Minibatch loss at step 1050: 0.8013657331466675\n",
      "Minibatch loss at step 1100: 0.8013584613800049\n",
      "Minibatch loss at step 1150: 0.8013513088226318\n",
      "Minibatch loss at step 1200: 0.8013443350791931\n",
      "Minibatch loss at step 1250: 0.8013371825218201\n",
      "Minibatch loss at step 1300: 0.8013303279876709\n",
      "Minibatch loss at step 1350: 0.8013234734535217\n",
      "Minibatch loss at step 1400: 0.8013166785240173\n",
      "Minibatch loss at step 1450: 0.8013099431991577\n",
      "Minibatch loss at step 1500: 0.8013032674789429\n",
      "Minibatch loss at step 1550: 0.8012967109680176\n",
      "Minibatch loss at step 1600: 0.8012902140617371\n",
      "Minibatch loss at step 1650: 0.8012837171554565\n",
      "Minibatch loss at step 1700: 0.8012773394584656\n",
      "Minibatch loss at step 1750: 0.8012710809707642\n",
      "Minibatch loss at step 1800: 0.8012648224830627\n",
      "Minibatch loss at step 1850: 0.8012586832046509\n",
      "Minibatch loss at step 1900: 0.8012524843215942\n",
      "Minibatch loss at step 1950: 0.8012464046478271\n",
      "Minibatch loss at step 2000: 0.8012403845787048\n",
      "Minibatch loss at step 2050: 0.8012345433235168\n",
      "Minibatch loss at step 2100: 0.8012287020683289\n",
      "Minibatch loss at step 2150: 0.8012228608131409\n",
      "Minibatch loss at step 2200: 0.8012170791625977\n",
      "Minibatch loss at step 2250: 0.801211416721344\n",
      "Minibatch loss at step 2300: 0.8012058138847351\n",
      "Minibatch loss at step 2350: 0.8012001514434814\n",
      "Minibatch loss at step 2400: 0.8011946082115173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2450: 0.801189124584198\n",
      "Minibatch loss at step 2500: 0.8011837005615234\n",
      "Minibatch loss at step 2550: 0.8011782169342041\n",
      "Minibatch loss at step 2600: 0.8011727333068848\n",
      "Minibatch loss at step 2650: 0.8011674284934998\n",
      "Minibatch loss at step 2700: 0.8011621832847595\n",
      "Minibatch loss at step 2750: 0.8011568188667297\n",
      "Minibatch loss at step 2800: 0.8011515140533447\n",
      "Minibatch loss at step 2850: 0.8011463284492493\n",
      "Minibatch loss at step 2900: 0.801141083240509\n",
      "Minibatch loss at step 2950: 0.8011360168457031\n",
      "Minibatch loss at step 3000: 0.8011307120323181\n",
      "Minibatch loss at step 3050: 0.8011257648468018\n",
      "Minibatch loss at step 3100: 0.8011206388473511\n",
      "Minibatch loss at step 3150: 0.8011155128479004\n",
      "Minibatch loss at step 3200: 0.8011106252670288\n",
      "Minibatch loss at step 3250: 0.8011054992675781\n",
      "Minibatch loss at step 3300: 0.8011006116867065\n",
      "Minibatch loss at step 3350: 0.8010956645011902\n",
      "Minibatch loss at step 3400: 0.8010907769203186\n",
      "Minibatch loss at step 3450: 0.8010859489440918\n",
      "Minibatch loss at step 3500: 0.801081120967865\n",
      "Minibatch loss at step 3550: 0.8010764718055725\n",
      "Minibatch loss at step 3600: 0.8010717630386353\n",
      "Minibatch loss at step 3650: 0.8010671138763428\n",
      "Minibatch loss at step 3700: 0.8010625839233398\n",
      "Minibatch loss at step 3750: 0.8010579943656921\n",
      "Minibatch loss at step 3800: 0.801053524017334\n",
      "Minibatch loss at step 3850: 0.8010489344596863\n",
      "Minibatch loss at step 3900: 0.8010445833206177\n",
      "Minibatch loss at step 3950: 0.8010401725769043\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i, vals in enumerate(learning_schedule):\n",
    "    ITERATIONS = vals[0]\n",
    "    LR = vals[1]\n",
    "    if i == 0:\n",
    "        model.learning_rate = LR\n",
    "        losses = model.train(x_train_flat, y_train_oh, TRAIN_BATCH_SIZE, ITERATIONS)\n",
    "    else:\n",
    "\n",
    "        losses_new = model.train(x_train_flat, y_train_oh, TRAIN_BATCH_SIZE, ITERATIONS, import_from_previous = True)\n",
    "        losses_new[:,0] += losses[-1,0] + TRAIN_BATCH_SIZE\n",
    "        losses = np.vstack((losses,losses_new))\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 109.79230809211731s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time: {}s\".format(end_time - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = losses[:,0]\n",
    "train_loss = losses[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "utls.remove_tex_axis(ax)\n",
    "ax.plot(iterations, train_loss,'-k')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.savefig('LC1_LR_sched.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Average test loss: 1.462615966796875\n"
     ]
    }
   ],
   "source": [
    "model.test(x_test_flat,y_test_oh, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Test sample digit: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "example = np.random.choice(n_test)\n",
    "\n",
    "sample = x_test_flat[example]\n",
    "label = y_test_oh[example]\n",
    "feed_dict = {model.input: np.expand_dims(sample, axis=0), \n",
    "             model.ground_truth: np.expand_dims(label,axis=0)}\n",
    "\n",
    "digit = np.where(label==1.0)[0][0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"./model.meta\")\n",
    "    saver.restore(sess, './model')\n",
    "    prediction = sess.run(model.prediction, feed_dict = feed_dict)[0]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(2*5,5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "ax = axs[0]\n",
    "utls.remove_tex_axis(ax)\n",
    "ax.imshow(x_test[example],cmap=cm.binary)\n",
    "ax.set_title('Test example')\n",
    "\n",
    "classes = np.arange(10)\n",
    "width = 0.5\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "ax.bar(classes, prediction, width, color='Black')\n",
    "utls.remove_tex_axis(ax,ytick_fmt=\"%.2f\")\n",
    "ax.set_xticks(classes)\n",
    "ax.set_xticklabels([str(x) for x in np.arange(10)])\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Digit class')\n",
    "ax.set_title('Network categorical distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "print('Test sample digit: {}'.format(digit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
